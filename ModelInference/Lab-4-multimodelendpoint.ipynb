{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon SageMaker Multi-Model Endpoints using MONAI Application Package container\n",
    "\n",
    "With [Amazon SageMaker Multi-Model Endpoints (MME)](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-model-endpoints.html), customers can create an endpoint that seamlessly hosts up to thousands of models, which can be served from a common inference container. It provides a scalable and cost-effective way to deploy large number of deep learning models. MME will run multiple models on a GPU core, share GPU instances behind an endpoint across multiple models and dynamically load/unload models based on the incoming traffic. With this, customers can significantly save cost and achieve best price performance. At a high level, Amazon SageMaker manages the loading and unloading of models for a multi-model endpoint, as they are needed. When an invocation request is made for a particular model, Amazon SageMaker routes the request to an instance assigned to that model, downloads the model artifacts from S3 onto that instance, and initiates loading of the model into the memory of the container. As soon as the loading is complete, Amazon SageMaker performs the requested invocation and returns the result. If the model is already loaded in memory on the selected instance, the downloading and loading steps are skipped and the invocation is performed immediately. For the inference container to serve multiple models in a multi-model endpoint, it must implement [additional APIs](https://docs.aws.amazon.com/sagemaker/latest/dg/build-multi-model-build-container.html) in order to load, list, get, unload and invoke specific models. [Example notebook](https://github.com/aws/amazon-sagemaker-examples/tree/main/advanced_functionality/multi_model_bring_your_own) is available for more implementation details.\n",
    "\n",
    "Let's start by creating a SageMaker session and specifying:\n",
    "- The S3 bucket and prefix that you want to use for the model. \n",
    "- The IAM role arn used to give hosting access to your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from botocore.exceptions import ClientError\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(service_name=\"sagemaker-runtime\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket = \"sagemaker-{}-{}\".format(region, account_id)\n",
    "prefix= \"medicalimaging-mms-map\"\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Inference Container\n",
    "\n",
    "We will use CI/CD pipeline to build the inference container with MONAI Application Package (MAP). The steps here followed the details in [this notebook](https://github.com/aws-samples/aws-research-workshops/blob/mainline/notebooks/container/container-cicd.ipynb).\n",
    "\n",
    "#### 1.0 Create IAM roles and ECR repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### We start by setting up the proper access permissions using the IAM service. Each service (CodePipeline, CodeBuild) needs its own policies. We also need to allow these services to access other related services on our behalf (S3 and ECR).\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "def create_service_role_with_policies(role_name, service_name, policy_arns):\n",
    "    try:\n",
    "        resp = iam_client.create_role(RoleName=role_name,\n",
    "                                 AssumeRolePolicyDocument='{\"Version\":\"2012-10-17\",\"Statement\":[{\"Sid\":\"\",\"Effect\":\"Allow\",\"Principal\":{\"Service\": \"' + service_name+'\"},\"Action\":\"sts:AssumeRole\"}]}')\n",
    "        for policy in policy_arns:\n",
    "            resp = iam_client.attach_role_policy(PolicyArn=policy,RoleName=role_name)\n",
    "    except ClientError  as e:\n",
    "        if e.response['Error']['Code'] == 'EntityAlreadyExists':\n",
    "            print(f\"{role_name} already exists, ignore\")\n",
    "        else: \n",
    "            raise  e\n",
    "    \n",
    "    resp = iam_client.get_role(RoleName=role_name)\n",
    "    return resp['Role']['Arn']\n",
    "\n",
    "\n",
    "codepipeline_service_role_name = f\"{prefix}-codepipeline-service-role\"\n",
    "codepipeline_policies = ['arn:aws:iam::aws:policy/AWSCodePipeline_FullAccess', \n",
    "                         'arn:aws:iam::aws:policy/AWSCodeCommitFullAccess',\n",
    "                         'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "                         'arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess'\n",
    "                        ]\n",
    "codepipeline_role_arn = create_service_role_with_policies(codepipeline_service_role_name, 'codepipeline.amazonaws.com', codepipeline_policies )\n",
    "print(f\"code pipeline IAM role ARN: {codepipeline_role_arn}\")\n",
    "              \n",
    "codebuild_service_role_name = f\"{prefix}-codebuild-service-role\"\n",
    "codebuild_policies = ['arn:aws:iam::aws:policy/AWSCodeBuildAdminAccess',\n",
    "                      'arn:aws:iam::aws:policy/CloudWatchFullAccess',\n",
    "                      'arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "                      'arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess']\n",
    "codebuild_role_arn = create_service_role_with_policies(codebuild_service_role_name, 'codebuild.amazonaws.com', codebuild_policies )\n",
    "print(f\"code build IAM role ARN: {codebuild_role_arn}\")\n",
    "\n",
    "\n",
    "### Before we can actually build our image, we need to have the repository referenced in our (post_build) phase. We will use boto3 again to interact with the AWS ECR APIs. We will actually use the repository after the container image is built.\n",
    "ecr_client = boto3.client(service_name=\"ecr\")\n",
    "fullname=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{prefix}:latest\"\n",
    "try:\n",
    "    res=ecr_client.describe_repositories(\n",
    "        repositoryNames=[\n",
    "            prefix\n",
    "        ]\n",
    "    )\n",
    "    print(f\"ECR repository ARN: {res['repositories'][0]['repositoryArn']}\")\n",
    "except:\n",
    "    print('Create new repository...')\n",
    "    res=ecr_client.create_repository(\n",
    "        repositoryName=prefix\n",
    "    )\n",
    "    print(f\"new ECR repository ARN: {res['repository']['repositoryArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Create an AWS CodeCommit repo and checkin the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# prepare the files for the checkin. \n",
    "with open(f\"{os.getcwd()}/src/Dockerfile\", \"r\") as f:\n",
    "    dockerfile_content = f.read()\n",
    "    \n",
    "with open(f\"{os.getcwd()}/src/model_handler.py\", \"r\") as f:\n",
    "    model_handler_content = f.read()\n",
    "    \n",
    "with open(f\"{os.getcwd()}/src/dockerd-entrypoint.py\", \"r\") as f:\n",
    "    entrypoint_content = f.read()\n",
    "\n",
    "with open(f\"{os.getcwd()}/src/buildspec.yml\", \"r\") as f:\n",
    "    buildspec_content = f.read()\n",
    "    \n",
    "with open(f\"{os.getcwd()}/src/code/app.py\", \"r\") as f:\n",
    "    app_content = f.read()\n",
    "    \n",
    "with open(f\"{os.getcwd()}/src/code/ahi_data_loader_operator.py\", \"r\") as f:\n",
    "    dataloader_content = f.read()\n",
    "    \n",
    "put_files=[\n",
    "    {\n",
    "        'filePath': 'Dockerfile',\n",
    "        'fileContent': dockerfile_content\n",
    "    },\n",
    "    {\n",
    "        'filePath': 'model_handler.py',\n",
    "        'fileContent': model_handler_content\n",
    "    },\n",
    "    {\n",
    "        'filePath': 'dockerd-entrypoint.py',\n",
    "        'fileContent': entrypoint_content\n",
    "    },\n",
    "    {\n",
    "        'filePath': 'buildspec.yml',\n",
    "        'fileContent': buildspec_content\n",
    "    },\n",
    "    {\n",
    "        'filePath': 'app.py',\n",
    "        'fileContent': app_content\n",
    "    },\n",
    "    {\n",
    "        'filePath': 'ahi_data_loader_operator.py',\n",
    "        'fileContent': dataloader_content\n",
    "    }\n",
    "]\n",
    "\n",
    "codecommit_client = boto3.client('codecommit')\n",
    "codecommit_name= f\"Source-{prefix}\"\n",
    "\n",
    "try:\n",
    "    resp = codecommit_client.create_repository(repositoryName=codecommit_name)\n",
    "    print(f\"new code repository: {res}\")\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'RepositoryNameExistsException':\n",
    "        print(f\"Repo {prefix} exists, use that one\")\n",
    "\n",
    "try:\n",
    "    resp = codecommit_client.get_branch(repositoryName=codecommit_name, branchName='main')\n",
    "    parent_commit_id = resp['branch']['commitId']\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'BranchDoesNotExistException':\n",
    "        # the repo is new, create it \n",
    "        codecommit_client.create_commit(repositoryName=codecommit_name, branchName=\"main\", putFiles=put_files)\n",
    "    else:\n",
    "        try:\n",
    "            resp = codecommit_client.create_commit(repositoryName=codecommit_name, branchName=\"main\", parentCommitId=parent_commit_id, putFiles=put_files) \n",
    "        except ClientError as ee:\n",
    "            if ee.response['Error']['Code'] == 'NoChangeException':\n",
    "                print('No change detected. skip commit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Create a CodeBuild project and CodePipeline to build container image\n",
    "We use an instance managed by AWS (see computeType below) to build the container using a standard Amazon Linux 2 build environment. The CodeBuild process is triggered by the CodeCommit code checkins.\n",
    "Note: codebuild-service-role takes a little longer to propagate. If you see a permission error, please retry again in a minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codebuild_client = boto3.client('codebuild')\n",
    "codebuild_name = f\"Build-{prefix}\" \n",
    "\n",
    "codepipeline_client = boto3.client('codepipeline')\n",
    "codepipeline_name = f\"Pipeline-{prefix}\" \n",
    "\n",
    "try: \n",
    "    resp = codebuild_client.create_project(\n",
    "        name=codebuild_name, \n",
    "        description=\"MMS Monai Deploy build demo\",\n",
    "        source= {\n",
    "            'type': \"CODEPIPELINE\"\n",
    "        },\n",
    "        artifacts= {\n",
    "            \"type\": \"CODEPIPELINE\",\n",
    "            \"name\": codepipeline_name\n",
    "        },\n",
    "        environment= {\n",
    "            \"type\": \"LINUX_CONTAINER\",\n",
    "            \"image\": \"aws/codebuild/amazonlinux2-x86_64-standard:3.0\",\n",
    "            \"computeType\": \"BUILD_GENERAL1_SMALL\",\n",
    "            \"environmentVariables\": [\n",
    "                {\n",
    "                    \"name\": \"AWS_DEFULT_REGION\",\n",
    "                    \"value\": region,\n",
    "                    \"type\": \"PLAINTEXT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"AWS_ACCOUNT_ID\",\n",
    "                    \"value\": account_id,\n",
    "                    \"type\": \"PLAINTEXT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"IMAGE_REPO_NAME\",\n",
    "                    \"value\": prefix,\n",
    "                    \"type\": \"PLAINTEXT\"\n",
    "                },\n",
    "                {\n",
    "                    \"name\": \"IMAGE_TAG\",\n",
    "                    \"value\": \"dev\",\n",
    "                    \"type\": \"PLAINTEXT\"\n",
    "                }\n",
    "            ],\n",
    "            \"privilegedMode\": True,\n",
    "            \"imagePullCredentialsType\": \"CODEBUILD\"               \n",
    "        },\n",
    "        logsConfig= {\n",
    "            \"cloudWatchLogs\": {\n",
    "                \"status\": \"ENABLED\",\n",
    "                \"groupName\": f\"log-{prefix}\"\n",
    "            },\n",
    "            \"s3Logs\": {\n",
    "                \"status\": \"DISABLED\"\n",
    "            }\n",
    "        },\n",
    "        serviceRole= codebuild_role_arn\n",
    "    )\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'ResourceAlreadyExistsException':\n",
    "        print(f\"CodeBuild project {codebuild_name} exists, skip...\")\n",
    "    else:\n",
    "        raise e\n",
    "        \n",
    "print(f\"CodeBuild project name {codebuild_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### create a pipeline with two stages commit and build.\n",
    "stage1 = {\n",
    "    \"name\":f\"{codecommit_name}\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"Source\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Source\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"provider\": \"CodeCommit\",\n",
    "                \"version\": \"1\"\n",
    "            },\n",
    "            \"runOrder\": 1,\n",
    "            \"configuration\": {\n",
    "                \"BranchName\": \"main\",\n",
    "                \"OutputArtifactFormat\": \"CODE_ZIP\",\n",
    "                \"PollForSourceChanges\": \"true\",\n",
    "                \"RepositoryName\": codecommit_name\n",
    "            },\n",
    "            \"outputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"SourceArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"inputArtifacts\": [],\n",
    "            \"region\": region,\n",
    "            \"namespace\": \"SourceVariables\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "stage2 = {\n",
    "   \"name\": f\"{codebuild_name}\",\n",
    "    \"actions\": [\n",
    "        {\n",
    "            \"name\": \"Build\",\n",
    "            \"actionTypeId\": {\n",
    "                \"category\": \"Build\",\n",
    "                \"owner\": \"AWS\",\n",
    "                \"provider\": \"CodeBuild\",\n",
    "                \"version\": \"1\"\n",
    "            },\n",
    "            \"runOrder\": 1,\n",
    "            \"configuration\": {\n",
    "                \"ProjectName\": codebuild_name\n",
    "            },\n",
    "            \"outputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"BuildArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"inputArtifacts\": [\n",
    "                {\n",
    "                    \"name\": \"SourceArtifact\"\n",
    "                }\n",
    "            ],\n",
    "            \"region\": region,\n",
    "            \"namespace\": \"BuildVariables\"\n",
    "        }\n",
    "    ]    \n",
    "}\n",
    "\n",
    "\n",
    "stages = [ stage1, stage2]\n",
    "\n",
    "\n",
    "pipeline = {\n",
    "    'name': codepipeline_name,\n",
    "    'roleArn': codepipeline_role_arn,\n",
    "    'artifactStore': {\n",
    "        'type': 'S3',\n",
    "        'location': bucket\n",
    "    }, \n",
    "    'stages': stages\n",
    "}\n",
    "\n",
    "try:\n",
    "    resp = codepipeline_client.create_pipeline( pipeline= pipeline)\n",
    "    print(\"Created pipeline\",resp)\n",
    "except ClientError as e:\n",
    "    if e.response['Error']['Code'] == 'PipelineNameInUseException':\n",
    "       print(f\"Codepipeline {codepipeline_name} already exists \" )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the container may take ~20 minutes, you can check if container is ready to use below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "while True:\n",
    "    resp = ecr_client.describe_images(repositoryName=prefix)\n",
    "    if resp['imageDetails']:\n",
    "        for image in resp['imageDetails']:\n",
    "            print(\"image pushed at: \" + str(image['imagePushedAt']))\n",
    "        break\n",
    "    else:\n",
    "        clear_output(wait=True)\n",
    "        display(\"Build not done yet, please wait and retry this step. Please do not proceed until you see the 'image pushed' message\")\n",
    "        time.sleep(20)\n",
    "# this is used later in job_definition for AWS Batch\n",
    "image_uri= f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{prefix}:dev\"\n",
    "print(image_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a multi-model inference endpoint\n",
    "#### 2.1 Create Model\n",
    "\n",
    "We will use SageMaker boto3 client [Create Model API](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreateModel.html) to create the Model entity for multi-model endpoints, with the container definition, ModelName, and ExecutionRoleArn.\n",
    "\n",
    "In the container definition, define the ModelDataUrl to specify the S3 directory that contains all the models that SageMaker multi-model endpoint will use to load and serve predictions. Set `Mode` to `MultiModel` to indicates SageMaker would create the endpoint with MME container specifications. We set the container with an image that supports deploying multi-model endpoints with GPU. The container's `ModelDataUrl` is the S3 prefix where the model artifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import gmtime, strftime\n",
    "\n",
    "### upload model to s3\n",
    "fObj = open(\"model.tar.gz\", \"rb\")\n",
    "key = os.path.join(prefix, \"model.tar.gz\")\n",
    "boto3.Session().resource(\"s3\").Bucket(bucket).Object(key).upload_fileobj(fObj)\n",
    "print(os.path.join(bucket, key))\n",
    "\n",
    "##############################################################################\n",
    "model_name = \"DEMO-MONAIDeployModel\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "model_url = \"s3://{}/{}/\".format(bucket, prefix)\n",
    "\n",
    "print(\"Model name: \" + model_name)\n",
    "print(\"Model data Url: \" + model_url)\n",
    "print(\"Container image: \" + image_uri)\n",
    "\n",
    "container = {\"Image\": image_uri, \"ModelDataUrl\": model_url, \"Mode\": \"MultiModel\"}\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name, ExecutionRoleArn=role, PrimaryContainer=container\n",
    ")\n",
    "\n",
    "print(\"Model Arn: \" + create_model_response[\"ModelArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint configuration\n",
    "Create a multi-model endpoint configurations using create_endpoint_config boto3 API. Specify an accelerated GPU computing instance in InstanceType, in this post we will use g4dn.xlarge instance. We recommend configuring your endpoints with at least two instances. This allows SageMaker to provide a highly available set of predictions across multiple Availability Zones for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = \"DEMO-MONAIDeployEndpointConfig-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint config name: \" + endpoint_config_name)\n",
    "\n",
    "create_endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"InstanceType\": \"ml.g4dn.xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"InitialVariantWeight\": 1,\n",
    "            \"ModelName\": model_name,\n",
    "            \"VariantName\": \"AllTraffic\",\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Endpoint config Arn: \" + create_endpoint_config_response[\"EndpointConfigArn\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint\n",
    "Using the above endpoint configuration we create a new sagemaker endpoint and wait for the deployment to finish. The status will change to InService once the deployment is successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = \"DEMO-MONAIDeployEndpoint-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\n",
    "print(\"Endpoint name: \" + endpoint_name)\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(\"Endpoint Arn: \" + create_endpoint_response[\"EndpointArn\"])\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "waiter = sm_client.get_waiter(\"endpoint_in_service\")\n",
    "print(\"Waiting for endpoint to create...\")\n",
    "waiter.wait(EndpointName=endpoint_name)\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "print(f\"Endpoint Status: {resp['EndpointStatus']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke models\n",
    "Once the endpoint is successfully created, we can send inference request to multi-model endpoint using invoke_enpoint API. We specify the TargetModel in the invocation call and pass in the payload for each model type. Sample invocation for PyTorch model and TensorRT model is shown below\n",
    "\n",
    "Now we invoke the models that we uploaded to S3 previously. The first invocation of a model may be slow, since behind the scenes, SageMaker is downloading the model artifacts from S3 to the instance and loading it into the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import json\n",
    "\n",
    "Payload = {\n",
    "    \"inputs\": [\n",
    "        {\"datastoreId\": datastoreId, \"imageSetId\": next(iter(imageSetIds))}\n",
    "    ]\n",
    "}\n",
    "\n",
    "response = runtime_sm_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    # ContentType=\"application/x-image\",\n",
    "    ContentType=\"application/json\",\n",
    "    Accept=\"application/json\",\n",
    "    TargetModel=\"model.tar.gz\",  # this is the rest of the S3 path where the model artifacts are located\n",
    "    Body=json.dumps(Payload)\n",
    ")\n",
    "\n",
    "print(*json.loads(response[\"Body\"].read()), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Delete the hosting resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "codepipeline_client.delete_pipeline(name=codepipeline_name)\n",
    "codebuild_client.delete_project(name=codebuild_name)\n",
    "codecommit_client.delete_repository(repositoryName=codecommit_name)\n",
    "ecr_client.delete_repository(repositoryName=prefix, force=True)\n",
    "\n",
    "def delete_service_role_with_policies(role_name, policy_arns):\n",
    "    iam_client = boto3.client('iam')\n",
    "    try:\n",
    "        for policy in policy_arns:\n",
    "            try: \n",
    "                resp = iam_client.detach_role_policy(PolicyArn=policy,RoleName=role_name)\n",
    "            except ClientError as ee:\n",
    "                if ee.response['Error']['Code'] == 'NoSuchEntity':\n",
    "                    print(\"Policy not attached, ignore\")\n",
    "                    \n",
    "        resp = iam_client.delete_role(RoleName=role_name)\n",
    "        print(f\"deleted service role {role_name}\")\n",
    "    except ClientError  as e:\n",
    "        if e.response['Error']['Code'] == 'NoSuchEntity':\n",
    "            print(f\"{role_name} already deleted, ignore\")\n",
    "        else: \n",
    "            raise  e\n",
    "            \n",
    "delete_service_role_with_policies(codepipeline_service_role_name, codepipeline_policies )\n",
    "delete_service_role_with_policies(codebuild_service_role_name, codebuild_policies )\n",
    "\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.12 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.12-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
